---
# Training
name: G2P
num_runs: 2
epochs: 10
batch_size: 1
training:
  clip_norm: 5.0
  optimizer: adam
  optimizer_params: {}
  use_gradient_clipping: true
curriculum: random-fair
early_stopping:
  metric: neg_avg_edit_distance
  patience: 5
  task_name: G2P_Celex

# Features
use_variational_dropout: true
label_connections: true
short_cut_connections: true
character_level_information:
  dimensionality: 100
  hidden_units: 100
  network_type: LSTM

# Network settings
units: 100
use_bias: true
rnn_unit: LSTM
# Dropout
word_dropout_keep_probability: 0.80
rnn_dropout_input_keep_probability: 0.80 
rnn_dropout_output_keep_probability: 0.80 
rnn_dropout_state_keep_probability: 0.80

# Evaluation
eval_metrics:
- accuracy
- word_accuracy
- avg_edit_distance

# Tasks
tasks:
- name: G2P_Celex
  # Data
  data_format: CONLL
  train_file:
    column_separator: tab
    encoding: utf8
    label_column: 1
    path: "../examples/g2p/data/celex/train.conll"
    scheme: IOB
    word_column: 0
  dev_file:
    column_separator: tab
    encoding: utf8
    label_column: 1
    path: "../examples/g2p/data/celex/dev.conll"
    scheme: IOB
    word_column: 0
  test_file:
    column_separator: tab
    encoding: utf8
    label_column: 1
    path: "../examples/g2p/data/celex/test.conll"
    scheme: IOB
    word_column: 0
  # Training
  classifier: softmax
  dropout_keep_probability: 0.8
  output_layer: 1
  use_bias: true
